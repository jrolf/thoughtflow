{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a0ddc27",
   "metadata": {},
   "source": [
    "# Script 01 — Foundations: LLM, MEMORY, THOUGHT\n",
    "\n",
    "This script teaches the three core primitives of ThoughtFlow from scratch.\n",
    "By the end you will understand how to:\n",
    "\n",
    "- Connect to language models (local and cloud)\n",
    "- Store and retrieve conversation state in MEMORY\n",
    "- Create THOUGHTs that reason over memory with real LLM calls\n",
    "\n",
    "**Every cell makes real API calls** — no mocks, no fakes.\n",
    "\n",
    "Prerequisites:\n",
    "  - Ollama running locally with a model pulled (`ollama pull llama3.2`)\n",
    "  - A Groq API key (free at https://console.groq.com)\n",
    "  - `pip install thoughtflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20d274b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 env vars\n"
     ]
    }
   ],
   "source": [
    "# --- Environment setup ---\n",
    "# Load API keys from .env and make thoughtflow importable.\n",
    "\n",
    "from _setup import load_env, print_heading, print_separator\n",
    "\n",
    "env = load_env()\n",
    "print(\"Loaded {} env vars\".format(len(env)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e33a118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thoughtflow import LLM, MEMORY, THOUGHT\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b61089f",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1 — The LLM Primitive\n",
    "\n",
    "`LLM` is a unified interface for language models.  You create one by\n",
    "passing a **provider:model** string.  Supported providers include\n",
    "`openai`, `anthropic`, `groq`, `gemini`, `openrouter`, and `ollama`.\n",
    "\n",
    "We will start with **Ollama** (a free, local model) to prove that\n",
    "ThoughtFlow works without any cloud dependency, then switch to\n",
    "**Groq** for the rest of the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a73524ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  1.1  LLM with Ollama (local)\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Provider : ollama\n",
      "Model    : gemma3\n",
      "Response : Python is a versatile, high-level programming language known for its readability and wide range of applications, from web development to data science.\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"1.1  LLM with Ollama (local)\")\n",
    "\n",
    "# Ollama requires no API key — it runs on your machine.\n",
    "# Make sure Ollama is running: `ollama serve`\n",
    "ollama_llm = LLM(\"ollama:gemma3\")\n",
    "\n",
    "# The simplest possible call: pass a list of message dicts.\n",
    "response = ollama_llm.call([\n",
    "    {\"role\": \"user\", \"content\": \"In one sentence, what is Python?\"}\n",
    "])\n",
    "\n",
    "print(\"Provider :\", ollama_llm.service)\n",
    "print(\"Model    :\", ollama_llm.model)\n",
    "print(\"Response :\", response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a48768b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  1.2  LLM with Groq (cloud)\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Provider : groq\n",
      "Model    : llama-3.3-70b-versatile\n",
      "Response : A good API is one that is well-documented, intuitive, and follows standard design principles, such as RESTful architecture, to provide a simple, secure, and scalable interface for interacting with a system or service.\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"1.2  LLM with Groq (cloud)\")\n",
    "\n",
    "# Groq hosts open-source models with very fast inference.\n",
    "groq_key = os.environ.get(\"GROQ_API_KEY\", \"\")\n",
    "llm = LLM(\"groq:llama-3.3-70b-versatile\", key=groq_key)\n",
    "\n",
    "response = llm.call([\n",
    "    {\"role\": \"user\", \"content\": \"In one sentence, what makes a good API?\"}\n",
    "])\n",
    "\n",
    "print(\"Provider :\", llm.service)\n",
    "print(\"Model    :\", llm.model)\n",
    "print(\"Response :\", response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf64e324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  1.3  Message normalization\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Dict messages : Blue.\n",
      "String message: Green.\n",
      "Mixed messages: Blue.\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"1.3  Message normalization\")\n",
    "\n",
    "# LLM.call() accepts several message formats.  It normalizes them\n",
    "# internally so you do not have to worry about the exact shape.\n",
    "\n",
    "# Format A: list of dicts (standard)\n",
    "resp_a = llm.call([\n",
    "    {\"role\": \"system\", \"content\": \"You are terse.  Reply in ≤5 words.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What color is the sky?\"},\n",
    "])\n",
    "print(\"Dict messages :\", resp_a[0])\n",
    "\n",
    "# Format B: plain strings (auto-wrapped as role='user')\n",
    "resp_b = llm.call([\"What color is grass?  Reply in ≤5 words.\"])\n",
    "print(\"String message:\", resp_b[0])\n",
    "\n",
    "# Format C: mixed — dicts and strings together\n",
    "resp_c = llm.call([\n",
    "    {\"role\": \"system\", \"content\": \"You are terse.  Reply in ≤5 words.\"},\n",
    "    \"What color is the ocean?\",\n",
    "])\n",
    "print(\"Mixed messages:\", resp_c[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4c129a",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2 — The MEMORY Primitive\n",
    "\n",
    "`MEMORY` is an **event-sourced state container**.  Every change — a\n",
    "message, a variable update, a log entry, a reflection — is stored as\n",
    "an immutable event with a unique, sortable stamp.\n",
    "\n",
    "Think of it as the agent's brain: it holds the full conversation,\n",
    "all working variables, and a complete audit trail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2516456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  2.1  Creating a MEMORY and adding messages\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Memory ID: 51yDVOIsQpd6Slo0\n",
      "\n",
      "Messages in memory:\n",
      "  [system] You are a helpful research assistant.\n",
      "  [user] Tell me about event sourcing.\n",
      "  [assistant] Event sourcing stores every state change as an immutable event.\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"2.1  Creating a MEMORY and adding messages\")\n",
    "\n",
    "memory = MEMORY()\n",
    "print(\"Memory ID:\", memory.id)\n",
    "\n",
    "# Messages require a role and content.  The optional `channel` parameter\n",
    "# tracks where the message came from (webapp, cli, telegram, etc.).\n",
    "memory.add_msg(\"system\", \"You are a helpful research assistant.\", channel=\"cli\")\n",
    "memory.add_msg(\"user\", \"Tell me about event sourcing.\", channel=\"cli\")\n",
    "memory.add_msg(\"assistant\", \"Event sourcing stores every state change as an immutable event.\", channel=\"cli\")\n",
    "\n",
    "print(\"\\nMessages in memory:\")\n",
    "for msg in memory.get_msgs():\n",
    "    print(\"  [{role}] {content}\".format(**msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36010d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  2.2  Convenience accessors\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Last user msg    : Tell me about event sourcing.\n",
      "Last assistant msg: Event sourcing stores every state change as an immutable event.\n",
      "Last system msg  : You are a helpful research assistant.\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"2.2  Convenience accessors\")\n",
    "\n",
    "# Quick access to the most recent message by role.\n",
    "print(\"Last user msg    :\", memory.last_user_msg(content_only=True))\n",
    "print(\"Last assistant msg:\", memory.last_asst_msg(content_only=True))\n",
    "print(\"Last system msg  :\", memory.last_sys_msg(content_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea6485ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  2.3  Variables — set, get, describe\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "topic        : event sourcing\n",
      "depth        : introductory\n",
      "sources_found: 0\n",
      "\n",
      "Description of 'topic': The user's research topic\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"2.3  Variables — set, get, describe\")\n",
    "\n",
    "# Variables are the agent's working memory.  Each set_var call\n",
    "# *appends* to the variable's history — it never overwrites.\n",
    "memory.set_var(\"topic\", \"event sourcing\", desc=\"The user's research topic\")\n",
    "memory.set_var(\"depth\", \"introductory\", desc=\"How deep the user wants to go\")\n",
    "memory.set_var(\"sources_found\", 0, desc=\"Number of sources found so far\")\n",
    "\n",
    "print(\"topic        :\", memory.get_var(\"topic\"))\n",
    "print(\"depth        :\", memory.get_var(\"depth\"))\n",
    "print(\"sources_found:\", memory.get_var(\"sources_found\"))\n",
    "\n",
    "# Descriptions are tracked separately and are retrievable.\n",
    "print(\"\\nDescription of 'topic':\", memory.get_var_desc(\"topic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54343585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  2.4  Variable history\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Current value: 12\n",
      "\n",
      "Full history of 'sources_found':\n",
      "  51yDVr70jTYx → 0\n",
      "  51yDW5khKuiY → 3\n",
      "  51yDW5kjYAty → 7\n",
      "  51yDW5kjUXJ2 → 12\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"2.4  Variable history\")\n",
    "\n",
    "# Every update to a variable is recorded.  Let us update sources_found\n",
    "# a few times and then inspect the full history.\n",
    "memory.set_var(\"sources_found\", 3)\n",
    "memory.set_var(\"sources_found\", 7)\n",
    "memory.set_var(\"sources_found\", 12)\n",
    "\n",
    "print(\"Current value:\", memory.get_var(\"sources_found\"))\n",
    "print(\"\\nFull history of 'sources_found':\")\n",
    "for stamp, value in memory.get_var_history(\"sources_found\"):\n",
    "    print(\"  {} → {}\".format(stamp[:12], value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f67c0247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  2.5  get_all_vars — snapshot of current state\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "All current variables:\n",
      "  topic = event sourcing\n",
      "  depth = introductory\n",
      "  sources_found = 12\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"2.5  get_all_vars — snapshot of current state\")\n",
    "\n",
    "all_vars = memory.get_all_vars()\n",
    "print(\"All current variables:\")\n",
    "for key, value in all_vars.items():\n",
    "    print(\"  {} = {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d660841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  2.6  Tombstone deletion\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "After deletion:\n",
      "  get_var('depth')       : None\n",
      "  is_var_deleted('depth'): True\n",
      "\n",
      "Full history of 'depth':\n",
      "  51yDVr6zu9Oi → introductory\n",
      "  51yDWQ2s2c3J → <DELETED>\n",
      "\n",
      "After re-setting: get_var('depth') = intermediate\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"2.6  Tombstone deletion\")\n",
    "\n",
    "# Deleting a variable does not erase it — it appends a tombstone marker.\n",
    "# The full history is always preserved.\n",
    "memory.del_var(\"depth\")\n",
    "\n",
    "print(\"After deletion:\")\n",
    "print(\"  get_var('depth')       :\", memory.get_var(\"depth\"))\n",
    "print(\"  is_var_deleted('depth'):\", memory.is_var_deleted(\"depth\"))\n",
    "\n",
    "# The history shows the deletion event.\n",
    "print(\"\\nFull history of 'depth':\")\n",
    "for stamp, value in memory.get_var_history(\"depth\"):\n",
    "    label = \"<DELETED>\" if value is not None and str(value).startswith(\"__\") else str(value)\n",
    "    print(\"  {} → {}\".format(stamp[:12], label))\n",
    "\n",
    "# Re-setting a deleted variable brings it back.\n",
    "memory.set_var(\"depth\", \"intermediate\")\n",
    "print(\"\\nAfter re-setting: get_var('depth') =\", memory.get_var(\"depth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b167c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  2.7  Multi-channel messages\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Messages from 'ios':\n",
      "  [user] Hey from my phone!\n",
      "\n",
      "Messages from 'telegram':\n",
      "  [user] Checking in on Telegram.\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"2.7  Multi-channel messages\")\n",
    "\n",
    "# Channels track where a message originated.  This matters when your\n",
    "# agent talks to users across multiple platforms simultaneously.\n",
    "memory.add_msg(\"user\", \"Hey from my phone!\", channel=\"ios\")\n",
    "memory.add_msg(\"user\", \"Checking in on Telegram.\", channel=\"telegram\")\n",
    "\n",
    "# Filter messages by channel.\n",
    "print(\"Messages from 'ios':\")\n",
    "for msg in memory.get_msgs(channel=\"ios\"):\n",
    "    print(\"  [{role}] {content}\".format(**msg))\n",
    "\n",
    "print(\"\\nMessages from 'telegram':\")\n",
    "for msg in memory.get_msgs(channel=\"telegram\"):\n",
    "    print(\"  [{role}] {content}\".format(**msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47fa0ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  2.8  Logs and reflections\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Logs:\n",
      "   Session started for user demo\n",
      "   Research topic identified: event sourcing\n",
      "\n",
      "Reflections:\n",
      "   The user seems interested in distributed systems concepts\n",
      "   Should ask about their experience level next\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"2.8  Logs and reflections\")\n",
    "\n",
    "# Logs are internal bookkeeping.  Reflections are the agent's\n",
    "# self-observations (useful for metacognitive architectures).\n",
    "memory.add_log(\"Session started for user demo\")\n",
    "memory.add_log(\"Research topic identified: event sourcing\")\n",
    "memory.add_ref(\"The user seems interested in distributed systems concepts\")\n",
    "memory.add_ref(\"Should ask about their experience level next\")\n",
    "\n",
    "print(\"Logs:\")\n",
    "for log in memory.get_logs():\n",
    "    print(\"  \", log[\"content\"])\n",
    "\n",
    "print(\"\\nReflections:\")\n",
    "for ref in memory.get_refs():\n",
    "    print(\"  \", ref[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2ed5fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  2.9  Rendering memory for LLMs\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Conversation render (first 500 chars):\n",
      "User: Tell me about event sourcing.\n",
      "\n",
      "Assistant: Event sourcing stores every state change as an immutable event.\n",
      "\n",
      "User: Hey from my phone!\n",
      "\n",
      "User: Checking in on Telegram.\n",
      "\n",
      "\n",
      "Prepared context (4 messages):\n",
      "  [user] Tell me about event sourcing.\n",
      "  [assistant] Event sourcing stores every state change as an immutable eve...\n",
      "  [user] Hey from my phone!\n",
      "  [user] Checking in on Telegram.\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"2.9  Rendering memory for LLMs\")\n",
    "\n",
    "# render() with format='conversation' produces a compact text blob\n",
    "# optimized for stuffing into an LLM prompt.\n",
    "conversation_text = memory.render(format=\"conversation\", max_total_length=1000)\n",
    "print(\"Conversation render (first 500 chars):\")\n",
    "print(conversation_text[:500])\n",
    "\n",
    "# prepare_context() gives you an OpenAI-compatible message list\n",
    "# with smart truncation of older messages.\n",
    "context = memory.prepare_context(recent_count=4, format=\"openai\")\n",
    "print(\"\\n\\nPrepared context ({} messages):\".format(len(context)))\n",
    "for msg in context:\n",
    "    preview = msg[\"content\"][:60]\n",
    "    if len(msg[\"content\"]) > 60:\n",
    "        preview += \"...\"\n",
    "    print(\"  [{}] {}\".format(msg[\"role\"], preview))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca4aa383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  2.10  Persistence — save and load\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "JSON export: 9203 chars\n",
      "Restored from JSON — topic: event sourcing\n",
      "\n",
      "Saved to: /var/folders/vy/hdl322p54t5fg8ftxtl6hdl40000gn/T/demo_memory.pkl\n",
      "Loaded from pickle — topic: event sourcing\n",
      "Loaded from pickle — events: 17\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"2.10  Persistence — save and load\")\n",
    "\n",
    "import tempfile, os\n",
    "\n",
    "# JSON round-trip (portable, human-readable)\n",
    "json_str = memory.to_json()\n",
    "print(\"JSON export: {} chars\".format(len(json_str)))\n",
    "\n",
    "restored_from_json = MEMORY.from_json(json_str)\n",
    "print(\"Restored from JSON — topic:\", restored_from_json.get_var(\"topic\"))\n",
    "\n",
    "# Pickle round-trip (fast, binary)\n",
    "tmp_path = os.path.join(tempfile.gettempdir(), \"demo_memory.pkl\")\n",
    "memory.save(tmp_path)\n",
    "print(\"\\nSaved to:\", tmp_path)\n",
    "\n",
    "loaded_memory = MEMORY()\n",
    "loaded_memory.load(tmp_path)\n",
    "print(\"Loaded from pickle — topic:\", loaded_memory.get_var(\"topic\"))\n",
    "print(\"Loaded from pickle — events:\", len(loaded_memory.events))\n",
    "\n",
    "# Clean up\n",
    "os.remove(tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0190aebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  2.11  Snapshot and rehydration\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Snapshot keys: ['id', 'events', 'objects']\n",
      "Event count  : 17\n",
      "Rehydrated OK — same topic? True\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"2.11  Snapshot and rehydration\")\n",
    "\n",
    "# snapshot() exports the full state.  from_events() rebuilds it.\n",
    "# This is the mechanism that enables cloud sync.\n",
    "snap = memory.snapshot()\n",
    "print(\"Snapshot keys:\", list(snap.keys()))\n",
    "print(\"Event count  :\", len(snap[\"events\"]))\n",
    "\n",
    "rehydrated = MEMORY.from_events(snap[\"events\"].values(), memory.id, objects=snap[\"objects\"])\n",
    "print(\"Rehydrated OK — same topic?\", rehydrated.get_var(\"topic\") == memory.get_var(\"topic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a666a39",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3 — The THOUGHT Primitive\n",
    "\n",
    "A `THOUGHT` is the atomic unit of cognition:\n",
    "\n",
    "> **THOUGHT = Prompt + Context + LLM + Parsing + Validation**\n",
    "\n",
    "You define *what* the agent should think about (the prompt), and\n",
    "THOUGHT handles building the messages, calling the LLM, parsing the\n",
    "response, validating it, and retrying if necessary.\n",
    "\n",
    "The universal pattern: `memory = thought(memory)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ea53a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  3.1  A simple THOUGHT\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Response:\n",
      "Event sourcing is an architectural pattern that involves storing the history of an application's state as a sequence of events, allowing for the reconstruction of the current state by replaying these events in the correct order. By capturing all changes to the application's state as discrete events, event sourcing enables auditing, versioning, and debugging, as well as flexible and scalable data processing and retrieval.\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"3.1  A simple THOUGHT\")\n",
    "\n",
    "# Start with a fresh memory for clarity.\n",
    "mem = MEMORY()\n",
    "mem.add_msg(\"user\", \"Explain event sourcing in 2 sentences.\", channel=\"cli\")\n",
    "\n",
    "# Create a THOUGHT.  The prompt uses {last_user_msg} which THOUGHT\n",
    "# automatically resolves from memory.\n",
    "respond = THOUGHT(\n",
    "    name=\"respond\",\n",
    "    llm=llm,\n",
    "    prompt=\"You are a knowledgeable software architect. Answer clearly: {last_user_msg}\",\n",
    ")\n",
    "\n",
    "# Execute it — this calls the LLM and stores the result.\n",
    "mem = respond(mem)\n",
    "\n",
    "# The result is stored in memory under \"{name}_result\".\n",
    "result = mem.get_var(\"respond_result\")\n",
    "print(\"Response:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3731c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  3.2  Prompt templating with variables\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Explanation:\n",
      "The concept of decorators in Python refers to a special kind of function that can modify or extend the behavior of another function. Decorators are defined with the '@' symbol followed by the name of the decorator function, and they are typically used to add additional functionality to existing functions without changing their source code. In Python, decorators are a powerful tool for implementing aspects such as logging, authentication, and caching, among others.\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"3.2  Prompt templating with variables\")\n",
    "\n",
    "# Prompts can reference any variable in memory via {variable_name}.\n",
    "mem = MEMORY()\n",
    "mem.set_var(\"language\", \"Python\")\n",
    "mem.set_var(\"concept\", \"decorators\")\n",
    "mem.add_msg(\"user\", \"Teach me about this topic.\", channel=\"cli\")\n",
    "\n",
    "explain = THOUGHT(\n",
    "    name=\"explain\",\n",
    "    llm=llm,\n",
    "    prompt=(\n",
    "        \"Explain the concept of {concept} in {language}. \"\n",
    "        \"Keep it to 3 sentences maximum.\"\n",
    "    ),\n",
    "    required_vars=[\"language\", \"concept\"],\n",
    ")\n",
    "\n",
    "mem = explain(mem)\n",
    "print(\"Explanation:\")\n",
    "print(mem.get_var(\"explain_result\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ebd02ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  3.3  Structured output with parsing_rules\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Extracted info:\n",
      "  name = Alice\n",
      "  age = 32\n",
      "  location = Portland\n",
      "  specialty = distributed systems\n",
      "  years_experience = 8\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"3.3  Structured output with parsing_rules\")\n",
    "\n",
    "# parsing_rules uses valid_extract to pull structured data from the\n",
    "# LLM's free-text response.  You specify the shape you want and\n",
    "# ThoughtFlow extracts it.\n",
    "\n",
    "mem = MEMORY()\n",
    "mem.set_var(\"text\", (\n",
    "    \"Alice is a 32-year-old software engineer from Portland. \"\n",
    "    \"She specializes in distributed systems and has 8 years of experience.\"\n",
    "))\n",
    "\n",
    "extract_info = THOUGHT(\n",
    "    name=\"extract\",\n",
    "    llm=llm,\n",
    "    prompt=(\n",
    "        \"Extract structured information from this text: {text}\\n\\n\"\n",
    "        \"Return a Python dict with keys: name, age, location, specialty, years_experience\"\n",
    "    ),\n",
    "    parsing_rules={\"kind\": \"python\", \"format\": {\n",
    "        \"name\": \"\",\n",
    "        \"age\": 0,\n",
    "        \"location\": \"\",\n",
    "        \"specialty\": \"\",\n",
    "        \"years_experience\": 0,\n",
    "    }},\n",
    "    max_retries=3,\n",
    ")\n",
    "\n",
    "mem = extract_info(mem)\n",
    "result = mem.get_var(\"extract_result\")\n",
    "print(\"Extracted info:\")\n",
    "for key, value in result.items():\n",
    "    print(\"  {} = {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c57c0589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  3.4  Validation — built-in validators\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Benefits (5):\n",
      "  1. Audit trail and history\n",
      "  2. Improved debugging and error handling\n",
      "  3. Easier implementation of undo/redo functionality\n",
      "  4. Better support for concurrency and parallel processing\n",
      "  5. Enhanced data analytics and reporting capabilities\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"3.4  Validation — built-in validators\")\n",
    "\n",
    "# Validators ensure the parsed result meets your requirements.\n",
    "# If validation fails, THOUGHT retries automatically.\n",
    "\n",
    "mem = MEMORY()\n",
    "mem.add_msg(\"user\", \"List 5 benefits of event sourcing.\", channel=\"cli\")\n",
    "\n",
    "list_benefits = THOUGHT(\n",
    "    name=\"benefits\",\n",
    "    llm=llm,\n",
    "    prompt=\"List exactly 5 benefits of event sourcing as a Python list of strings: {last_user_msg}\",\n",
    "    parsing_rules={\"kind\": \"python\", \"format\": []},\n",
    "    validator=\"list_min_len:5\",\n",
    "    max_retries=3,\n",
    ")\n",
    "\n",
    "mem = list_benefits(mem)\n",
    "result = mem.get_var(\"benefits_result\")\n",
    "print(\"Benefits ({}):\".format(len(result)))\n",
    "for i, benefit in enumerate(result, 1):\n",
    "    print(\"  {}. {}\".format(i, benefit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "380078dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  3.5  Validation — custom validator function\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Haiku about machine learning:\n",
      "Algorithms dance\n",
      "Learning from the data sea\n",
      "Intelligence born\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"3.5  Validation — custom validator function\")\n",
    "\n",
    "# You can pass a callable as the validator for full control.\n",
    "# It must return (True, \"\") on success or (False, \"reason\") on failure.\n",
    "\n",
    "def validate_haiku(result):\n",
    "    \"\"\"Check that the result has exactly 3 lines (a haiku).\"\"\"\n",
    "    if not isinstance(result, str):\n",
    "        return False, \"Expected a string\"\n",
    "    lines = [l for l in result.strip().split(\"\\n\") if l.strip()]\n",
    "    if len(lines) != 3:\n",
    "        return False, \"Haiku must have exactly 3 lines, got {}\".format(len(lines))\n",
    "    return True, \"\"\n",
    "\n",
    "mem = MEMORY()\n",
    "mem.set_var(\"subject\", \"machine learning\")\n",
    "\n",
    "write_haiku = THOUGHT(\n",
    "    name=\"haiku\",\n",
    "    llm=llm,\n",
    "    prompt=(\n",
    "        \"Write a haiku about {subject}. \"\n",
    "        \"Output ONLY the three lines of the haiku, nothing else.\"\n",
    "    ),\n",
    "    validation=validate_haiku,\n",
    "    max_retries=3,\n",
    ")\n",
    "\n",
    "mem = write_haiku(mem)\n",
    "print(\"Haiku about machine learning:\")\n",
    "print(mem.get_var(\"haiku_result\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4f6b32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  3.6  Non-LLM operations — memory_query\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Gathered context (no LLM call):\n",
      "  user_name = Alice\n",
      "  session_id = sess_001\n",
      "  topic = event sourcing\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"3.6  Non-LLM operations — memory_query\")\n",
    "\n",
    "# Not every THOUGHT needs an LLM.  The memory_query operation\n",
    "# retrieves variables from memory without making any API call.\n",
    "\n",
    "mem = MEMORY()\n",
    "mem.set_var(\"user_name\", \"Alice\")\n",
    "mem.set_var(\"session_id\", \"sess_001\")\n",
    "mem.set_var(\"topic\", \"event sourcing\")\n",
    "\n",
    "gather_context = THOUGHT(\n",
    "    name=\"context\",\n",
    "    operation=\"memory_query\",\n",
    "    required_vars=[\"user_name\", \"session_id\", \"topic\"],\n",
    ")\n",
    "\n",
    "mem = gather_context(mem)\n",
    "context = mem.get_var(\"context_result\")\n",
    "print(\"Gathered context (no LLM call):\")\n",
    "for key, value in context.items():\n",
    "    print(\"  {} = {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8d755f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  3.7  Non-LLM operations — variable_set\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Variables after init:\n",
      "  session_active = True\n",
      "  turn_count = 0\n",
      "  research_phase = discovery\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"3.7  Non-LLM operations — variable_set\")\n",
    "\n",
    "# The variable_set operation writes values into memory.\n",
    "# Useful for initializing state at the start of a workflow.\n",
    "\n",
    "mem = MEMORY()\n",
    "\n",
    "init_session = THOUGHT(\n",
    "    name=\"init\",\n",
    "    operation=\"variable_set\",\n",
    "    prompt={\n",
    "        \"session_active\": True,\n",
    "        \"turn_count\": 0,\n",
    "        \"research_phase\": \"discovery\",\n",
    "    },\n",
    ")\n",
    "\n",
    "mem = init_session(mem)\n",
    "print(\"Variables after init:\")\n",
    "for key, value in mem.get_all_vars().items():\n",
    "    if not key.endswith(\"_result\"):\n",
    "        print(\"  {} = {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a443d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  3.8  Pre/post hooks\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "  [hook] Turn 1 complete — status: OK\n",
      "\n",
      "Response: CQRS (Command Query Responsibility Segregation) is a software design pattern that separates an application's responsibil ...\n",
      "Turn count: 1\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"3.8  Pre/post hooks\")\n",
    "\n",
    "# Hooks let you run custom logic before or after a THOUGHT executes.\n",
    "\n",
    "def before_thought(thought, memory, vars, **kwargs):\n",
    "    \"\"\"Increment the turn counter before each execution.\"\"\"\n",
    "    current = memory.get_var(\"turn_count\") or 0\n",
    "    memory.set_var(\"turn_count\", current + 1)\n",
    "\n",
    "def after_thought(thought, memory, result, error):\n",
    "    \"\"\"Print a summary after execution.\"\"\"\n",
    "    turn = memory.get_var(\"turn_count\")\n",
    "    status = \"OK\" if error is None else \"ERROR: {}\".format(error)\n",
    "    print(\"  [hook] Turn {} complete — status: {}\".format(turn, status))\n",
    "\n",
    "mem = MEMORY()\n",
    "mem.set_var(\"turn_count\", 0)\n",
    "mem.add_msg(\"user\", \"What is CQRS?\", channel=\"cli\")\n",
    "\n",
    "hooked_thought = THOUGHT(\n",
    "    name=\"hooked\",\n",
    "    llm=llm,\n",
    "    prompt=\"Answer briefly: {last_user_msg}\",\n",
    "    pre_hook=before_thought,\n",
    "    post_hook=after_thought,\n",
    ")\n",
    "\n",
    "mem = hooked_thought(mem)\n",
    "print(\"\\nResponse:\", mem.get_var(\"hooked_result\")[:120], \"...\")\n",
    "print(\"Turn count:\", mem.get_var(\"turn_count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43305520",
   "metadata": {},
   "source": [
    "---\n",
    "## Recap\n",
    "\n",
    "You have now used all three foundation primitives:\n",
    "\n",
    "| Primitive | Purpose | Key pattern |\n",
    "|-----------|---------|-------------|\n",
    "| **LLM** | Unified interface to language models | `response = llm.call(messages)` |\n",
    "| **MEMORY** | Event-sourced state container | `memory.set_var(k, v)` / `memory.get_var(k)` |\n",
    "| **THOUGHT** | Atomic unit of cognition | `memory = thought(memory)` |\n",
    "\n",
    "Next: **Script 02** — Decisions, Plans, and Actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1118e2f-803b-4f7d-8b8b-a38dbfc425a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af4592b-7638-472f-a9a8-33a100fbff90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3950e48f-0a24-494f-9035-6974260fba85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12f030c6-1f5a-477f-b49f-62e1ac8abebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[END!]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed3c0be-6a02-4805-93e0-e6480df9d365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
