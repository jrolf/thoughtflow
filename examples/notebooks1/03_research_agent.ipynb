{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2f68199",
   "metadata": {},
   "source": [
    "# Script 03 — Building a Conversational Research Agent\n",
    "\n",
    "This script combines every primitive from Scripts 01 and 02 into a\n",
    "single, interactive research agent with personality and intelligence.\n",
    "\n",
    "The agent:\n",
    "- **Asks clarifying questions** before diving into research\n",
    "- **Decides** what the user wants on every turn (DECIDE)\n",
    "- **Plans** its research approach (PLAN)\n",
    "- **Searches** the web and **fetches** content (SEARCH, FETCH)\n",
    "- **Synthesizes** findings with its own perspective (THOUGHT)\n",
    "- **Remembers** everything across turns (MEMORY)\n",
    "- **Chats** interactively with the user (CHAT)\n",
    "\n",
    "Prerequisites:\n",
    "  - Groq API key and Brave Search API key in `.env`\n",
    "  - `pip install thoughtflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "097e7bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from _setup import load_env, print_heading, print_separator\n",
    "import os\n",
    "\n",
    "env = load_env()\n",
    "\n",
    "from thoughtflow import (\n",
    "    LLM, MEMORY, THOUGHT, DECIDE, PLAN,\n",
    "    SEARCH, CHAT,\n",
    ")\n",
    "\n",
    "llm = LLM(\"groq:llama-3.3-70b-versatile\", key=os.environ.get(\"GROQ_API_KEY\", \"\"))\n",
    "brave_key = os.environ.get(\"BRAVE_API_KEY\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f5c6e4",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1 — Agent Architecture\n",
    "\n",
    "The agent operates in phases, controlled by intent classification:\n",
    "\n",
    "| Intent | What happens |\n",
    "|--------|-------------|\n",
    "| `new_topic` | Agent asks clarifying questions to understand the request |\n",
    "| `clarify` | Agent synthesizes the user's answers and refines its understanding |\n",
    "| `research` | Agent plans searches, fetches results, synthesizes findings |\n",
    "| `deeper` | Agent digs into a sub-topic from previous results |\n",
    "| `done` | Agent wraps up with a closing summary |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfec383f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  1.1  System prompt — the agent's persona\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "System prompt loaded (1043 chars)\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"1.1  System prompt — the agent's persona\")\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are Ariel, a sharp and curious research analyst.\n",
    "\n",
    "Your personality:\n",
    "- You are direct and opinionated — you share your own synthesis, not just a dump of search results.\n",
    "- You ask probing questions before researching. You want to understand WHAT the person actually needs, not just the surface request.\n",
    "- When you find conflicting information, you surface the conflict honestly rather than glossing over it.\n",
    "- You are concise but thorough. You respect people's time.\n",
    "- You have a slight dry wit, but you never sacrifice clarity for humor.\n",
    "\n",
    "Your workflow:\n",
    "- When someone brings a new topic, ask 2-3 sharp clarifying questions FIRST.\n",
    "- Only after you understand their angle do you dive into research.\n",
    "- When presenting findings, lead with your own take, then support it with evidence.\n",
    "- Always tell the user when you are about to search, so they know what is happening.\n",
    "\n",
    "Rules:\n",
    "- Never fabricate sources. If you searched and found nothing useful, say so.\n",
    "- If you are unsure, say so and explain why.\n",
    "- Keep responses focused. No walls of text.\"\"\"\n",
    "\n",
    "print(\"System prompt loaded ({} chars)\".format(len(SYSTEM_PROMPT)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dbf725",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2 — Building the Agent's Components\n",
    "\n",
    "Each component is a reusable primitive that the agent function\n",
    "orchestrates at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7111dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  2.1  Intent classifier (DECIDE)\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Intent classifier ready: Decide: intent (choices: new_topic, clarify, research, deeper, done)\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"2.1  Intent classifier (DECIDE)\")\n",
    "\n",
    "# This runs on every user turn to figure out what they want.\n",
    "intent_classifier = DECIDE(\n",
    "    name=\"intent\",\n",
    "    llm=llm,\n",
    "    choices={\n",
    "        \"new_topic\": \"User is introducing a new research topic or question\",\n",
    "        \"clarify\": \"User is answering a clarifying question or providing more context\",\n",
    "        \"research\": \"User wants me to go ahead and research now\",\n",
    "        \"deeper\": \"User wants to dig deeper into a sub-topic from previous results\",\n",
    "        \"done\": \"User wants to wrap up or says goodbye\",\n",
    "    },\n",
    "    prompt=(\n",
    "        \"Based on the conversation so far and the user's latest message, \"\n",
    "        \"determine their intent.\\n\\n\"\n",
    "        \"Conversation context:\\n{conversation_context}\\n\\n\"\n",
    "        \"Latest user message: {last_user_msg}\"\n",
    "    ),\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    max_retries=3,\n",
    ")\n",
    "\n",
    "print(\"Intent classifier ready:\", intent_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45744b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  2.2  Discovery thought (asks clarifying questions)\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Discovery thought ready: Thought: discovery\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"2.2  Discovery thought (asks clarifying questions)\")\n",
    "\n",
    "discovery_thought = THOUGHT(\n",
    "    name=\"discovery\",\n",
    "    llm=llm,\n",
    "    prompt=(\n",
    "        \"The user wants to research: {last_user_msg}\\n\\n\"\n",
    "        \"What they have told you so far:\\n{conversation_context}\\n\\n\"\n",
    "        \"Ask 2-3 sharp, specific clarifying questions that will help you \"\n",
    "        \"understand exactly what angle of this topic they care about. \"\n",
    "        \"Be conversational and direct — you are Ariel.\"\n",
    "    ),\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    ")\n",
    "\n",
    "print(\"Discovery thought ready:\", discovery_thought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b0fef96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  2.3  Research planner (PLAN)\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Research planner ready: Plan: research_plan (3 actions available)\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"2.3  Research planner (PLAN)\")\n",
    "\n",
    "research_planner = PLAN(\n",
    "    name=\"research_plan\",\n",
    "    llm=llm,\n",
    "    actions={\n",
    "        \"search\": {\n",
    "            \"description\": \"Search the web for information on a specific query\",\n",
    "            \"params\": {\"query\": \"str\"},\n",
    "        },\n",
    "        \"analyze\": {\n",
    "            \"description\": \"Analyze and extract key insights from gathered information\",\n",
    "            \"params\": {\"focus\": \"str\"},\n",
    "        },\n",
    "        \"synthesize\": {\n",
    "            \"description\": \"Create a clear synthesis of all findings\",\n",
    "            \"params\": {\"format\": \"str?\"},\n",
    "        },\n",
    "    },\n",
    "    prompt=(\n",
    "        \"The user wants to know about: {research_topic}\\n\"\n",
    "        \"Specific angle: {research_angle}\\n\\n\"\n",
    "        \"Create a focused research plan.  Use 2-3 search queries that \"\n",
    "        \"cover different angles of the topic.  Then analyze and synthesize.\"\n",
    "    ),\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    max_steps=5,\n",
    "    max_parallel=2,\n",
    ")\n",
    "\n",
    "print(\"Research planner ready:\", research_planner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b389db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  2.4  Synthesis thought (final answer)\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Synthesis thought ready: Thought: synthesis\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"2.4  Synthesis thought (final answer)\")\n",
    "\n",
    "synthesis_thought = THOUGHT(\n",
    "    name=\"synthesis\",\n",
    "    llm=llm,\n",
    "    prompt=(\n",
    "        \"You are presenting your research findings to the user.\\n\\n\"\n",
    "        \"Topic: {research_topic}\\n\"\n",
    "        \"Angle: {research_angle}\\n\\n\"\n",
    "        \"Search results:\\n{search_findings}\\n\\n\"\n",
    "        \"Conversation so far:\\n{conversation_context}\\n\\n\"\n",
    "        \"Present your findings in a clear, opinionated way. \"\n",
    "        \"Lead with your own synthesis, then support with evidence. \"\n",
    "        \"If you found conflicting information, surface it. \"\n",
    "        \"Keep it focused and actionable.\"\n",
    "    ),\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    ")\n",
    "\n",
    "print(\"Synthesis thought ready:\", synthesis_thought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "742e933f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  2.5  Deeper-dive thought\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Deeper-dive thought ready: Thought: deeper_dive\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"2.5  Deeper-dive thought\")\n",
    "\n",
    "deeper_thought = THOUGHT(\n",
    "    name=\"deeper_dive\",\n",
    "    llm=llm,\n",
    "    prompt=(\n",
    "        \"The user wants to go deeper on a sub-topic.\\n\\n\"\n",
    "        \"Their request: {last_user_msg}\\n\"\n",
    "        \"Previous research context:\\n{search_findings}\\n\\n\"\n",
    "        \"Identify the specific sub-topic they want to explore, \"\n",
    "        \"then provide a more detailed analysis.  Use what you already \"\n",
    "        \"know from previous research, and note what additional searching \"\n",
    "        \"might be needed.\"\n",
    "    ),\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    ")\n",
    "\n",
    "print(\"Deeper-dive thought ready:\", deeper_thought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c84deb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  2.6  Wrap-up thought\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Wrap-up thought ready: Thought: wrapup\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"2.6  Wrap-up thought\")\n",
    "\n",
    "wrapup_thought = THOUGHT(\n",
    "    name=\"wrapup\",\n",
    "    llm=llm,\n",
    "    prompt=(\n",
    "        \"The user is wrapping up the conversation.\\n\\n\"\n",
    "        \"Conversation:\\n{conversation_context}\\n\\n\"\n",
    "        \"Give a brief, warm closing.  If you researched something, \"\n",
    "        \"offer a 1-2 sentence takeaway.  Keep it short.\"\n",
    "    ),\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    ")\n",
    "\n",
    "print(\"Wrap-up thought ready:\", wrapup_thought)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebce72f",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3 — The Agent Function\n",
    "\n",
    "The agent function follows the ThoughtFlow contract: it takes a MEMORY\n",
    "and returns a MEMORY.  Internally it classifies intent, then dispatches\n",
    "to the appropriate phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "257de3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  3.1  Defining the agent function\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "research_agent() defined — ready to use.\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"3.1  Defining the agent function\")\n",
    "\n",
    "\n",
    "def research_agent(memory):\n",
    "    \"\"\"\n",
    "    A conversational research agent that classifies user intent on each\n",
    "    turn and dispatches to discovery, research, deep-dive, or wrap-up.\n",
    "\n",
    "    Follows the ThoughtFlow contract: memory in, memory out.\n",
    "\n",
    "    The agent tracks its phase and accumulated findings in memory\n",
    "    variables so it can maintain context across turns.\n",
    "    \"\"\"\n",
    "    # Build a conversation summary for the LLM to reference.\n",
    "    conversation_context = memory.render(\n",
    "        format=\"conversation\",\n",
    "        max_total_length=3000,\n",
    "    )\n",
    "    memory.set_var(\"conversation_context\", conversation_context)\n",
    "\n",
    "    # --- Classify intent ---\n",
    "    memory = intent_classifier(memory)\n",
    "    intent = memory.get_var(\"intent_result\")\n",
    "\n",
    "    # --- Dispatch based on intent ---\n",
    "\n",
    "    if intent == \"new_topic\":\n",
    "        # Store the topic for later use.\n",
    "        memory.set_var(\"research_topic\", memory.last_user_msg(content_only=True))\n",
    "        memory.set_var(\"research_phase\", \"discovery\")\n",
    "        # Ask clarifying questions.\n",
    "        memory = discovery_thought(memory)\n",
    "        agent_response = memory.get_var(\"discovery_result\")\n",
    "\n",
    "    elif intent == \"clarify\":\n",
    "        # The user answered our questions.  Refine our understanding.\n",
    "        memory.set_var(\"research_angle\", memory.last_user_msg(content_only=True))\n",
    "        memory.set_var(\"research_phase\", \"ready_to_research\")\n",
    "        # Acknowledge and tell them we are about to search.\n",
    "        agent_response = (\n",
    "            \"Got it — that gives me a clear direction.  \"\n",
    "            \"Let me search for some current information on this.  One moment...\"\n",
    "        )\n",
    "\n",
    "    elif intent == \"research\":\n",
    "        # Execute the research pipeline.\n",
    "        agent_response = _execute_research(memory)\n",
    "\n",
    "    elif intent == \"deeper\":\n",
    "        # Dig deeper on a sub-topic.\n",
    "        memory = deeper_thought(memory)\n",
    "        agent_response = memory.get_var(\"deeper_dive_result\")\n",
    "\n",
    "    elif intent == \"done\":\n",
    "        memory = wrapup_thought(memory)\n",
    "        agent_response = memory.get_var(\"wrapup_result\")\n",
    "\n",
    "    else:\n",
    "        agent_response = \"I am not sure what you mean. Could you rephrase?\"\n",
    "\n",
    "    # Store the agent's response as an assistant message.\n",
    "    memory.add_msg(\"assistant\", agent_response, channel=\"cli\")\n",
    "\n",
    "    return memory\n",
    "\n",
    "\n",
    "def _execute_research(memory):\n",
    "    \"\"\"\n",
    "    Run the full research pipeline: plan → search → synthesize.\n",
    "\n",
    "    Returns the synthesized response string.\n",
    "    \"\"\"\n",
    "    # Ensure we have a topic and angle.\n",
    "    topic = memory.get_var(\"research_topic\") or memory.last_user_msg(content_only=True)\n",
    "    angle = memory.get_var(\"research_angle\") or \"\"\n",
    "    memory.set_var(\"research_topic\", topic)\n",
    "    memory.set_var(\"research_angle\", angle)\n",
    "    memory.set_var(\"research_phase\", \"researching\")\n",
    "\n",
    "    # Step 1: Generate a research plan.\n",
    "    memory = research_planner(memory)\n",
    "    plan = memory.get_var(\"research_plan_result\") or []\n",
    "\n",
    "    # Step 2: Execute search queries from the plan.\n",
    "    all_snippets = []\n",
    "    search_queries = []\n",
    "\n",
    "    for step in plan:\n",
    "        for task in step:\n",
    "            if task.get(\"action\") == \"search\":\n",
    "                query = task.get(\"params\", {}).get(\"query\", topic)\n",
    "                search_queries.append(query)\n",
    "\n",
    "    # Run up to 3 searches.\n",
    "    for query in search_queries[:3]:\n",
    "        searcher = SEARCH(\n",
    "            name=\"live_search\",\n",
    "            provider=\"brave\",\n",
    "            query=query,\n",
    "            api_key=brave_key,\n",
    "            max_results=4,\n",
    "        )\n",
    "        memory = searcher(memory)\n",
    "        results = memory.get_var(\"live_search_results\") or {}\n",
    "        for r in results.get(\"results\", []):\n",
    "            snippet = \"- [{}] {}\".format(r.get(\"title\", \"\"), r.get(\"snippet\", \"\"))\n",
    "            all_snippets.append(snippet)\n",
    "\n",
    "    # Store combined findings.\n",
    "    findings_text = \"\\n\".join(all_snippets) if all_snippets else \"(No results found.)\"\n",
    "    memory.set_var(\"search_findings\", findings_text)\n",
    "\n",
    "    # Step 3: Synthesize.\n",
    "    memory = synthesis_thought(memory)\n",
    "    response = memory.get_var(\"synthesis_result\")\n",
    "\n",
    "    memory.set_var(\"research_phase\", \"presented\")\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "print(\"research_agent() defined — ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f14a49",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4 — Running the Agent with CHAT\n",
    "\n",
    "The `CHAT` class wraps any ThoughtFlow-contract function and provides\n",
    "an interactive input/output loop.\n",
    "\n",
    "- `CHAT.run()` — blocking interactive loop (type `q` to exit)\n",
    "- `CHAT.turn(text)` — single turn, ideal for cell-by-cell use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36ebfc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  4.1  Programmatic turns with CHAT.turn()\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Ariel: Hi, I'm Ariel — your research analyst.  What would you like to explore?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"4.1  Programmatic turns with CHAT.turn()\")\n",
    "\n",
    "# CHAT.turn() lets you drive the conversation programmatically.\n",
    "# This is perfect for notebooks where you want to see each step.\n",
    "\n",
    "chat = CHAT(\n",
    "    research_agent,\n",
    "    greeting=\"Hi, I'm Ariel — your research analyst.  What would you like to explore?\",\n",
    "    channel=\"cli\",\n",
    "    user_label=\"You\",\n",
    "    agent_label=\"Ariel\",\n",
    ")\n",
    "\n",
    "# Display the greeting.\n",
    "print(\"Ariel:\", chat.greeting)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab5d06c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "──────────────────────────────────────────────────────────────────────\n",
      "You: I want to understand how companies are using LLMs in production right now.\n",
      "\n",
      "Ariel: So you want to know how companies are using Large Language Models (LLMs) in production. That's a pretty broad topic. To give you a more focused answer, can you tell me: \n",
      "\n",
      "1. Are you interested in a specific industry, like healthcare or finance, or do you want a general overview across multiple sectors?\n",
      "2. What aspect of LLM usage are you most curious about - is it the types of tasks they're being used for, the benefits they're bringing, or perhaps the challenges companies are facing in implementation?\n",
      "3. Are you looking for examples of well-known companies, or are you also interested in learning about smaller organizations or startups that might be using LLMs in innovative ways?\n"
     ]
    }
   ],
   "source": [
    "# Turn 1: Introduce a topic.\n",
    "print_separator()\n",
    "print(\"You: I want to understand how companies are using LLMs in production right now.\")\n",
    "response = chat.turn(\"I want to understand how companies are using LLMs in production right now.\")\n",
    "print()\n",
    "print(\"Ariel:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ae651fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "──────────────────────────────────────────────────────────────────────\n",
      "You: I'm mostly interested in the infrastructure and cost side — how are companies managing latency, reliability, and spending at scale?\n",
      "\n",
      "Ariel: research\n"
     ]
    }
   ],
   "source": [
    "# Turn 2: Answer clarifying questions.\n",
    "print_separator()\n",
    "user_msg = (\n",
    "    \"I'm mostly interested in the infrastructure and cost side — \"\n",
    "    \"how are companies managing latency, reliability, and spending at scale?\"\n",
    ")\n",
    "print(\"You:\", user_msg)\n",
    "response = chat.turn(user_msg)\n",
    "print()\n",
    "print(\"Ariel:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b86fa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "──────────────────────────────────────────────────────────────────────\n",
      "You: Yes, go ahead and research that.\n",
      "\n",
      "Ariel: Based on my research, it's clear that managing latency, reliability, and spending at scale is a significant challenge for companies using Large Language Models (LLMs) in production. My synthesis is that companies must carefully weigh the benefits of using LLMs against the significant infrastructure and cost challenges. \n",
      "\n",
      "To mitigate these expenses, companies are exploring techniques like caching, batching, and dynamic scaling to cut inference costs. For instance, using spot instances for non-urgent tasks and reserved instances for predictable workloads can help minimize costs. Additionally, optimizing memory usage, applying lower precision calculations, and configuring GPU features for asynchronous execution can also enhance performance.\n",
      "\n",
      "The estimated annual costs for a production-grade LLM deployment can range from $800,000 to $1,200,000, primarily due to human infrastructure costs. A conservative example using OpenAI pricing estimates a monthly cost of $7,500 to $9,000 for a pilot project with 20 prompts per day. However, it's essential to consider that these costs can vary widely depending on the specific use case, model size, and infrastructure requirements.\n",
      "\n",
      "To address latency, companies are leveraging features like predicted outputs, which can significantly reduce generation time when most of the output is known ahead of time. Auto-scaling with serverless endpoints to match demand spikes and dynamic routing that prioritizes workloads based on latency or other factors can also help optimize LLM performance.\n",
      "\n",
      "Overall, my findings suggest that companies using LLMs in production must prioritize cost-effective infrastructure management, latency optimization, and scalability to ensure reliable and efficient performance. By employing targeted optimization strategies and exploring cost-effective solutions, companies can better manage latency, reliability, and spending at scale.\n"
     ]
    }
   ],
   "source": [
    "# Turn 3: Ask the agent to go research.\n",
    "print_separator()\n",
    "print(\"You: Yes, go ahead and research that.\")\n",
    "response = chat.turn(\"Yes, go ahead and research that.\")\n",
    "print()\n",
    "print(\"Ariel:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ab125a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "──────────────────────────────────────────────────────────────────────\n",
      "You: Tell me more about the cost optimization strategies specifically.\n",
      "\n",
      "Ariel: The user wants to explore the sub-topic of cost optimization strategies for Large Language Models (LLMs) in production. Based on my previous research, I've identified some key strategies that companies are using to optimize costs, including:\n",
      "\n",
      "1. **Caching, batching, and dynamic scaling**: These techniques can help reduce inference costs by minimizing the number of requests made to the LLM.\n",
      "2. **Using spot instances for non-urgent tasks and reserved instances for predictable workloads**: This approach can help companies save on infrastructure costs by utilizing cheaper instance types for non-critical tasks.\n",
      "3. **Storing data and models in the same region**: This can help minimize transfer fees and reduce overall costs.\n",
      "4. **Auto-scaling with serverless endpoints**: This approach can help match demand spikes and reduce costs by only provisionally allocating resources when needed.\n",
      "5. **Dynamic routing**: This technique can help prioritize workloads based on latency or other factors, ensuring that critical tasks are processed efficiently.\n",
      "6. **Model pruning, quantization, and knowledge distillation**: These techniques can help reduce the computational requirements of LLMs, making them more efficient and cost-effective.\n",
      "7. **Scaling laws**: These laws can help forecast model behavior and avoid fully training every candidate model, reducing costs and improving efficiency.\n",
      "\n",
      "To provide a more detailed analysis, I would need to conduct additional research on the following topics:\n",
      "\n",
      "* **Cost-benefit analysis of different instance types**: I would need to investigate the costs and benefits of using different instance types, such as spot instances, reserved instances, and on-demand instances, to determine the most cost-effective approach.\n",
      "* **Optimization techniques for specific LLM architectures**: I would need to research optimization techniques that are specifically tailored to certain LLM architectures, such as transformer-based models or recurrent neural networks.\n",
      "* **Industry-specific cost optimization strategies**: I would need to investigate cost optimization strategies that are specific to certain industries, such as healthcare or finance, to provide more targeted advice.\n",
      "\n",
      "By conducting additional research on these topics, I can provide a more comprehensive and detailed analysis of cost optimization strategies for LLMs in production.\n"
     ]
    }
   ],
   "source": [
    "# Turn 4: Ask to go deeper.\n",
    "print_separator()\n",
    "print(\"You: Tell me more about the cost optimization strategies specifically.\")\n",
    "response = chat.turn(\"Tell me more about the cost optimization strategies specifically.\")\n",
    "print()\n",
    "print(\"Ariel:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69ff0d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "──────────────────────────────────────────────────────────────────────\n",
      "You: Great, that's very helpful. Thanks!\n",
      "\n",
      "Ariel: It was a pleasure helping you understand how companies are using LLMs in production. To recap, companies are employing various strategies to optimize infrastructure costs and reduce latency, with estimated annual costs for production-grade LLM deployments ranging from $800,000 to $1,200,000.\n"
     ]
    }
   ],
   "source": [
    "# Turn 5: Wrap up.\n",
    "print_separator()\n",
    "print(\"You: Great, that's very helpful. Thanks!\")\n",
    "response = chat.turn(\"Great, that's very helpful. Thanks!\")\n",
    "print()\n",
    "print(\"Ariel:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e436f0c",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5 — Inspecting Memory After the Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d292ca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  5.1  Conversation history\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Turns recorded: 5\n",
      "\n",
      "--- Turn 1 ---\n",
      "  You  : I want to understand how companies are using LLMs in production right now. \n",
      "  Ariel: So you want to know how companies are using Large Language Models (LLMs) in prod ...\n",
      "\n",
      "--- Turn 2 ---\n",
      "  You  : I'm mostly interested in the infrastructure and cost side — how are companies ma ...\n",
      "  Ariel: research \n",
      "\n",
      "--- Turn 3 ---\n",
      "  You  : Yes, go ahead and research that. \n",
      "  Ariel: Based on my research, it's clear that managing latency, reliability, and spendin ...\n",
      "\n",
      "--- Turn 4 ---\n",
      "  You  : Tell me more about the cost optimization strategies specifically. \n",
      "  Ariel: The user wants to explore the sub-topic of cost optimization strategies for Larg ...\n",
      "\n",
      "--- Turn 5 ---\n",
      "  You  : Great, that's very helpful. Thanks! \n",
      "  Ariel: It was a pleasure helping you understand how companies are using LLMs in product ...\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"5.1  Conversation history\")\n",
    "\n",
    "print(\"Turns recorded:\", len(chat.history))\n",
    "for i, (user_text, agent_text) in enumerate(chat.history):\n",
    "    print(\"\\n--- Turn {} ---\".format(i + 1))\n",
    "    print(\"  You  :\", user_text[:80], \"...\" if len(user_text) > 80 else \"\")\n",
    "    print(\"  Ariel:\", (agent_text or \"\")[:80], \"...\" if len(agent_text or \"\") > 80 else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ff5a9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  5.2  Memory variables\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "  research_topic = I want to understand how companies are using LLMs in production right now.\n",
      "  research_angle = \n",
      "  research_phase = presented\n",
      "  intent_result = done\n",
      "  research_plan_result = [[{'action': 'search', 'params': {'query': 'LLM production infrastructure costs'}, 'reason': 'Gather...\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"5.2  Memory variables\")\n",
    "\n",
    "# The agent stored various state variables during the conversation.\n",
    "interesting_vars = [\n",
    "    \"research_topic\", \"research_angle\", \"research_phase\",\n",
    "    \"intent_result\", \"research_plan_result\",\n",
    "]\n",
    "\n",
    "for var in interesting_vars:\n",
    "    value = chat.memory.get_var(var)\n",
    "    if value is not None:\n",
    "        preview = str(value)[:100]\n",
    "        if len(str(value)) > 100:\n",
    "            preview += \"...\"\n",
    "        print(\"  {} = {}\".format(var, preview))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9eb81cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "  5.3  All events in memory\n",
      "══════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Total events: 99\n",
      "Messages    : 22\n",
      "Variables   : 35\n",
      "Logs        : 30\n",
      "Reflections : 12\n"
     ]
    }
   ],
   "source": [
    "print_heading(\"5.3  All events in memory\")\n",
    "\n",
    "print(\"Total events:\", len(chat.memory.events))\n",
    "print(\"Messages    :\", len(chat.memory.idx_msgs))\n",
    "print(\"Variables   :\", len(chat.memory.idx_vars))\n",
    "print(\"Logs        :\", len(chat.memory.idx_logs))\n",
    "print(\"Reflections :\", len(chat.memory.idx_refs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1018313a",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6 — Interactive Mode (optional)\n",
    "\n",
    "Uncomment the cell below to run a fully interactive session.\n",
    "Type your messages, and the agent will respond.\n",
    "Type `q` or `quit` to exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ba064e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to run interactively:\n",
    "# interactive_chat = CHAT(\n",
    "#     research_agent,\n",
    "#     greeting=\"Hi, I'm Ariel — your research analyst.  What would you like to explore?\",\n",
    "#     channel=\"cli\",\n",
    "#     user_label=\"You\",\n",
    "#     agent_label=\"Ariel\",\n",
    "# )\n",
    "# interactive_chat.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af90dbdc",
   "metadata": {},
   "source": [
    "---\n",
    "## Recap\n",
    "\n",
    "In this script we built a complete agent from ThoughtFlow primitives:\n",
    "\n",
    "| Component | Primitive | Role |\n",
    "|-----------|-----------|------|\n",
    "| Intent classifier | DECIDE | Routes each turn to the right phase |\n",
    "| Discovery | THOUGHT | Asks clarifying questions |\n",
    "| Research planner | PLAN | Creates a multi-step search strategy |\n",
    "| Web search | SEARCH | Fetches real results from Brave |\n",
    "| Synthesis | THOUGHT | Produces an opinionated summary |\n",
    "| Deep dive | THOUGHT | Explores sub-topics in detail |\n",
    "| Wrap-up | THOUGHT | Closes the conversation gracefully |\n",
    "| Interactive loop | CHAT | Handles turn-taking and history |\n",
    "\n",
    "Next: **Script 04** — Deploy this agent to the cloud with ThoughtBase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd9c28d-bdef-4e9d-a017-00288d071a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dee9ee-efe5-4abf-862b-da9cdef4afd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea52e313-2008-4490-b7f3-4bbc1ff521bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc48fa3f-6f44-4700-bf56-bcc2349254bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55f7d946-59d3-4995-977e-b13e3ce39812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [END!]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3bfd6e-a52f-4489-8ba0-8f835f55a1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb81aa1-bf74-43b5-b0c5-090d16d41bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
